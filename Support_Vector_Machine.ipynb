{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 2: Support Vector Machine\n",
    "\n",
    "## Remarks:\n",
    "1. Please complete the theoretical questions directly in this notebook using `markdown`. We do not accept handwritten PA submissions.\n",
    "2. You will use `cvxopt` package in this assignment. To ensure consistency of notation, please use `pip install cvxopt==1.2.7` to install a suitable version (1.3.0 version may encounter problems)\n",
    "3. You will use `torch` to implement binary classification and `torchvision` to load MNIST dataset. `tqdm` is used to show the learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install cvxopt==1.2.7\n",
    "! pip install scikit-learn\n",
    "! pip install torch torchvision tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of Soft-margin SVM Optimization Problem\n",
    "A SVM computes a hyperplane classifier of the form $f(x)=w^{T}x+b$, where $w$ is the normal vector to the hyperplane, $x$ is the input vector and $b$ is the bias scalar. Since we apply it to a binary classification problem, we will ultimately predict label $y = 1$ if $f(x) ≥ 0$ and $y = −1$ if $f(x) < 0$. By deriving the dual form of the SVM problem, we convert the original non-convex problem into a Quadratic Programming (QP) problem over a set of Lagrange multipliers $\\alpha=\\{ \\alpha_1,\\alpha_2,\\ldots,\\alpha_N\\}$ . \n",
    "\n",
    "Consider a dataset with $N$ samples $\\{x_1,x_2,\\ldots,x_N\\}$. The soft-margin SVM algorithm tries to solve:\n",
    "\n",
    "$$\\begin{array}{rc}\\underset{\\alpha}{max} &\\sum\\limits^N_{i=1}\\alpha_i-\\frac{1}{2}\\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1}\\alpha_i\\alpha_jy_iy_jK(x_i,x_j) \\\\ \\text { s.t. } & \\sum\\limits^N_{i=1}\\alpha_iy_i=0, \\\\ & 0 \\leqslant \\alpha_i \\leqslant C,i=1,2,\\ldots,N.\\end{array}$$\n",
    "\n",
    "where $C$  is the penalty parameter. $K$ is the kernel function, which allow us to learn non-linear decision function through feature mapping. When an inner product is used (e.g. $K(x_i,x_j)=\\langle x_i,x_j\\rangle$ ), the model degenerates into a linear SVM. We can also represent the kernel function as a symmetric matrix $K_{i,j} = K(x_i, x_j)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the optimal solution $\\alpha^{*}$ , we could obtain the optimal $w^*$ and $b^*$ and then derive the decision function for classification: \n",
    "$f(x)=\\sum\\limits_{i=1}^{N} \\alpha_{i}^{*} y_{i} K\\left(x, x_{i}\\right)+b^{*}.$ The KKT conditions can be used to check for convergence to the optimal solution. In other words, any $\\alpha$ that satisfy these properties will be an optimal solution to the\n",
    "optimization problem given above. For this problem the KKT conditions are:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\alpha_i=0 & \\Rightarrow y^{(i)}\\left(w^T x^{(i)}+b\\right) \\geq 1 \\\\\n",
    "\\alpha_i=C & \\Rightarrow y^{(i)}\\left(w^T x^{(i)}+b\\right) \\leq 1 \\\\\n",
    "0<\\alpha_i<C & \\Rightarrow y^{(i)}\\left(w^T x^{(i)}+b\\right)=1.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will introduce two different ways of solving $\\alpha^{*}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: SVM as a Quadratic Programming Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving QP Problem with CVXOPT Package\n",
    "\n",
    "In this section, we will use a Python package `cvxopt` to see how SVM works. The `cvxopt` is widely used in solving optimization problems. We are also going to use `numpy`, `pandas`, `random`, `seaborn` and `matplotlib` packages to help us. The first thing we need to do is to import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import cvxopt\n",
    "from cvxopt import matrix, solvers\n",
    "from sklearn import datasets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QP solves quadratic optimization problems, to optimize a multivariate quadratic function subject with linear constraints on the variables. They can be solved via the solvers.qp() function in `cvxopt`. The standard form of a QP (following CVXOPT notation) is:\n",
    "\n",
    "$$\\begin{array}{rc}\\underset{x}{min}  & \\frac{1}{2} x^{\\mathrm{T}} P x+q^{\\mathrm{T}} x \\\\ \\text { s.t. } & G x \\preceq h, \\\\ & A x=b.\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. Before using it to solve SVM, we need to transform the soft-margin SVM optimization problem above into the standard form. Please derive corresponding $P,q,G,h,A,b$ here and show the derivation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your Answer:*\n",
    "$$P=\\\\ q= \\\\ G= \\\\ h= \\\\ A= \\\\ b= $$\n",
    "Derivation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. Now use `numpy` and `cvxopt` to implement soft-margin SVM algorithm in the form of QP problem. Check this document for its usage: https://cvxopt.org/examples/tutorial/qp.html. (Remark: the data type of matrix objects for qp function should be double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1,x2):\n",
    "    return np.dot(x1,x2)\n",
    "\n",
    "def kernel_matrix(X, n_samples, kernel):\n",
    "    K = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            K[i, j] = kernel(X[i], X[j])\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_qp(X, y, C , kernel=linear_kernel):\n",
    "    # X: feature matrix, X[i,:] is the vector x_i\n",
    "    # y: labels\n",
    "    # C: soft-margin parameter\n",
    "    # kernel: kernel function. default linear kernel\n",
    "    # return optimal alpha and b of SVM\n",
    "\n",
    "    N, n_features = X.shape\n",
    "    K = kernel_matrix(X, N, kernel)\n",
    "    \"\"\" Your Code here \"\"\"\n",
    "    P = \n",
    "    q = \n",
    "    G = \n",
    "    h = \n",
    "    A = \n",
    "    b = \n",
    "    \"\"\" Your Code here \"\"\"\n",
    "\n",
    "    # Solve Quadratic Programming problem:\n",
    "    solvers.options['maxiters'] = 200\n",
    "    solution = solvers.qp(P, q, G, h, A, b)\n",
    "    alphas = np.array(solution['x']).reshape(N) # results of qp\n",
    "    # Remark: In the result, some alphas that are infinitely close to zero in value should be considered as 0.\n",
    "    # You can set a threshold for that, e.g. 1e-4.\n",
    "    alphas[alphas<1e-4] = 0\n",
    "    # Calculate bias b in svm\n",
    "    \"\"\" Your Code here \"\"\"\n",
    "    b_svm = \n",
    "    \"\"\" Your Code here \"\"\"\n",
    "\n",
    "    return alphas, b_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(data, X, y, alphas, b, kernel):\n",
    "    # return decision function f(x) on vector point x=`data`\n",
    "    # X is feature matrix and y is the label list\n",
    "    # alphas and b are SVM outputs\n",
    "    \"\"\" Your Code here \"\"\"\n",
    "    \n",
    "    \"\"\" Your Code here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have implemented the SVM algorithm. Let's test it on a small dataset and plot the hyperplane for visulization of support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(X, y, alphas, b, kernel):\n",
    "    # visulize the decision hyperplane on the dataset\n",
    "    x_min = min(X[:, 0]) - 0.5 # get boundry of the figure, x y here are coordinate axis\n",
    "    x_max = max(X[:, 0]) + 0.5\n",
    "    y_min = min(X[:, 1]) - 0.5\n",
    "    y_max = max(X[:, 1]) + 0.5\n",
    "    step = 0.1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))\n",
    "    d = np.array([[x1, x2] for x1, x2 in zip(np.ravel(xx), np.ravel(yy))])\n",
    "    Z = np.zeros(len(d))\n",
    "    for i in range(len(d)): \n",
    "        Z[i] = f(d[i], X, y, alphas, b, kernel)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, ax=ax, palette='winter') # draw the data points\n",
    "    ax.contour(xx, yy, Z, levels=[-1, 0, 1]) # draw the hyperplane\n",
    "    plt.show()\n",
    "    \n",
    "X = np.array([[3, 4], [1, 4],[2, 3], [6, -1], [7, -1], [5, -3], [2, 4]])\n",
    "y = np.array([-1, -1, -1, 1, 1, 1, 1])\n",
    "alphas, b = svm_qp(X, y, C=0.1, kernel=linear_kernel)\n",
    "plot(X, y, alphas, b, kernel=linear_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. While the data can be classified by soft-margin SVM, we still don't know which parameter can realize the \"best\" performance. Run with different penalty parameters $C$ and try to find an \"optimal\" value. Compare and analyze you results, e.g. why it is an optimal parameter, what is happening as $C$ is getting larger(smaller)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. When the data set is not linearly separable, we need to utilize different kernel functions. Here is a non-linear dataset, please test different kernel functions (e.g. Polynomial kernel and RBF kernel) and choose one to fit a soft-margin SVM model on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate half moon data (ref: https://shadow-ssml.readthedocs.io/en/latest/examples/halfmoons_example.html)\n",
    "# parameters for data generation, you may modify them for further observation and comparison\n",
    "n_samples = 100  # number of samples to generate\n",
    "noise = 0.1  # noise to add to sample locations\n",
    "X, y = datasets.make_moons(n_samples=n_samples, noise=noise)\n",
    "y = (y - 0.5) * 2\n",
    "\n",
    "def your_kernel(x1, x2):\n",
    "    \"\"\" Your Code here \"\"\"\n",
    "    pass\n",
    "    \"\"\" Your Code here \"\"\"\n",
    "\n",
    "alphas, b = svm_qp(X, y, C=10, kernel=your_kernel)\n",
    "plot(X, y, alphas, b, kernel=your_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Hinge Loss for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beside SVM, here we also introduce Hinge Loss, which formulates the binary classification loss as:\n",
    "\\begin{equation}\n",
    "    L(y) = max(0, c - y \\cdot \\hat y),\n",
    "\\end{equation}\n",
    "where $c$ is a hyperparameter (usually $1$), $y$ is the true label ($\\pm 1$) and $\\hat y$ is the prediction ([-1, 1]). As the predictions get further and further away from the true labels, the loss also gets larger. Additionally, once the prediction is correct ($y \\cdot \\hat y$), the loss is $0$ to avoid overconfidence. \n",
    "\n",
    "Actually, using Hinge Loss for binary classfication is equivalent to soft-margin SVM by considering an L2 regularization. The problem of using Hinge Loss for binary classification can be formulated as:\n",
    "$$\n",
    "   \\text{minimize} \\frac{1}{N} \\sum_i max(0, 1 - y_i(W^T x_i + b)) + \\lambda ||W||^2.\n",
    "$$\n",
    "If you are familiar with soft-margin SVM, you will easily find the equivalence by denoting  $ \\zeta_i = max(0, 1 - y_i(W^T x_i + b))$:\n",
    "$$\n",
    "   \\text{minimize} \\frac{1}{N} \\sum_i \\zeta_i + \\lambda ||W||^2, \\\\\n",
    "   \\text{subject to} \\quad \\zeta_i > 0,\\quad  1 - y_i(W^T x_i + b) \\le \\zeta_i \\quad \\text{for all $i$},\n",
    "$$\n",
    "which is exactly the formulation of soft-margin SVM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use Hinge Loss as the loss function of Stochastic Gradient Descent (SGD) method to classfy 0 and 1 in MNIST. (If you are not familiar with the mechanism of auto differentiation, you can refer to https://d2l.ai/chapter_preliminaries/autograd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device =  \"cpu\" # we will use cpu in this assignment\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                torchvision.transforms.Normalize(mean = [0.5],std = [0.5])])\n",
    "path = './data/'\n",
    "TRAIN_DATA = torchvision.datasets.MNIST(path, train = True,transform = transform, download = True)\n",
    "VAL_DATA = torchvision.datasets.MNIST(path, train = False,transform = transform, download = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only data labelled as 0 and 1 is needed, so we preprocess data with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, label, num_samples):\n",
    "    # Filter the dataset to include only the specified labels\n",
    "    # data: the original dataset\n",
    "    # label(list): needed labels, e.g. [0, 1]\n",
    "    # num_samples(list): samples for each label\n",
    "    \n",
    "    indices = [i for i, (img, lbl) in enumerate(data) if lbl in label]\n",
    "    filtered_data = torch.utils.data.Subset(data, indices)\n",
    "    \n",
    "    # Further reduce the dataset to the specified number of samples per label\n",
    "    label_counts = {lbl: 0 for lbl in label}\n",
    "    selected_indices = []\n",
    "    \n",
    "    for i in indices:\n",
    "        _, lbl = data[i]\n",
    "        if label_counts[lbl] < num_samples[label.index(lbl)]:\n",
    "            selected_indices.append(i)\n",
    "            label_counts[lbl] += 1\n",
    "        if all(count >= num_samples[label.index(lbl)] for count in label_counts.values()):\n",
    "            break\n",
    "    \n",
    "    final_data = torch.utils.data.Subset(data, selected_indices)\n",
    "    \n",
    "    # Create DataLoader for the preprocessed data\n",
    "    data_loader = torch.utils.data.DataLoader(final_data, batch_size=1, shuffle=True)\n",
    "    \n",
    "    return final_data, data_loader\n",
    "\n",
    "train_data, train_data_loader = prepare_data(TRAIN_DATA, label=[0,1], num_samples=[200,200])\n",
    "val_data, val_data_loader = prepare_data(VAL_DATA, label=[0,1], num_samples=[50,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5: Then we need to implement Hinge Loss in Equation(1) function with `Torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(preds, targets, c=1):\n",
    "    # calculate hinge loss with torch tensor\n",
    "    # preds: Batch_size x 1, predicted scores\n",
    "    # targets: Batch_size x 1, ground truth labels\n",
    "    # loss: Real number, hinge loss for preds and targets\n",
    "    \"\"\" Your Code here \"\"\"\n",
    "    loss=\n",
    "    \"\"\" Your Code here \"\"\"\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6(Optional): Finally implement the training loop. (It might take a minute for training.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some hyperparameters\n",
    "lr=1e-4\n",
    "epochs=80\n",
    "\n",
    "# initialize linear model\n",
    "# requires_grad = True to make w and b trainable\n",
    "w=torch.randn(784,1,requires_grad=True)\n",
    "b=torch.randn(1,requires_grad=True)\n",
    "\n",
    "# optimizer\n",
    "optimizer=torch.optim.SGD([w,b],lr=lr)\n",
    "\n",
    "losses=[]\n",
    "accuracy=[]\n",
    "# training\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    avg_loss = 0\n",
    "    for i,(img,lbl) in enumerate(train_data_loader):\n",
    "        # forward propogate\n",
    "        # calulate the predicted scores and loss\n",
    "        # You may use the hinge_loss function in Q5\n",
    "        # hint: to ensure the robustness of the model, you may add L2 regularization term to the loss function \n",
    "        # \n",
    "        \"\"\" Your Code here \"\"\"\n",
    "        loss=\n",
    "        \"\"\" Your Code here \"\"\"\n",
    "        # backward propogate\n",
    "        # update the weights and bias using torch optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss+=loss.item()\n",
    "    # record average loss at each epoch\n",
    "    avg_loss/=len(train_data_loader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    # validation\n",
    "    # test the model on validation set and calculate the accuracy at each epoch\n",
    "    avg_accu = 0\n",
    "    # When testing, we need to shut requires_grad_ down to prevent the model being trained on the validation set\n",
    "    w.requires_grad_(False)\n",
    "    b.requires_grad_(False)\n",
    "    # torch.no_grad shuts down the automatic calculation of gradients\n",
    "    with torch.no_grad():\n",
    "        for i,(img,lbl) in enumerate(val_data_loader):\n",
    "            img=img.view(-1,784)\n",
    "            preds=torch.matmul(img,w)+b\n",
    "            preds = (preds.squeeze() > 0).int()\n",
    "            accu = (preds == lbl).float().mean()  \n",
    "            avg_accu += accu.item()\n",
    "    # record the testing accuracy at each epoch \n",
    "    avg_accu /= len(val_data_loader)    \n",
    "    accuracy.append(avg_accu)\n",
    "    # after validation, we need to make w and b trainable again\n",
    "    w.requires_grad_(True)\n",
    "    b.requires_grad_(True)\n",
    "\n",
    "# plot the training loss and validation accuracy\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(accuracy)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7(Optional): Replace Hinge Loss with [Binary Cross-Entropy(BCE) Loss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) and compare the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
